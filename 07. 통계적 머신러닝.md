# 통계적 머신러닝

- 최근 통계학 분야는 예측 모델링을 자동화하기 위한 기술을 개발하는 것에 집중됨
    - 이러한 방법들은 통계적 머신러닝(statistical machine learning)라는 큰 틀에 속함
    - 데이터 기반이며, 전체적인 구조를 가정하지 않는다는 점에서 정통통계와 구별됨(ex. 모델이 선형인지)

## K-최근접 이웃(KNN, K-Nearest Neighbors)

용어 정리

- 이웃(neighbor): 예측 변수에서 값들이 유사한 레코드
- 거리 지표(distance metric): 각 레코드 사이가 얼마나 멀리 떨어져있는지 나타내는 단일값
- 표준화(=정규화, standardization): 평균을 뺀 후, 표준편차로 나누는 절차
- z점수(z-score): 표준화를 통해 얻은 값
- K: 최근접 이웃을 계산하는 데 사용되는 이웃의 수

---

- knn 알고리즘의 아이디어(절차)
    1. 예측변수들이 유사한 K개의 레코드를 찾는다.
    2. 분류) 1. 에서 찾은 유사한 K개의 레코드들 중, 다수가 속한 클래스로 분류한다.
    3. 예측) 유사한 K개의 레코드들의 평균을 예측값으로 사용
- 모델 피팅이 필요 없음
- 완전 자동화는 아님, 아래와 같은 것들을 설정해야함(파라미터)
    - 특징들이 어떤 척도에 존재하는지,
    - 가까운 정도의 측정은 어떻게 할 것인지,
    - K개를 어떻게 설정할 것인지
- 모든 예측변수들은 수치형이어야한다.
- 이진분류에서 KNN 모델을 활용할 경우,
    - K=20이고,
    - 레이블0: 12 // 레이블1: 8 일 때,
    - 레이블0일 확률은 12/20 = 0.6이다.
    - 근데, 5장에서 살펴본 것처럼 레이블1이 소수인 희귀데이터일 경우,
    - 레이블1로 분류할 컷오프를 50% 미만으로 설정하여 모델링하는 것이 더 정확할 수 있다.

### 거리지표

- 유사성(similarity) (=근접성, nearness)은 거리지표를 통해 결정됨
- 두 데이터간 거리를 측정하는 함수이며, 가장 많이 사용되는 지표는 **유클리드 거리(Euclidean distancs)**
    - 유클리드 거리: 두 벡터 사이의 직선상 거리
- 맨하탄 거리(Manhattan distance)도 자주 쓰임
    - 점과 점 사이의 이동시간으로 근접시간을 따질 때 좋은 지표가 된다.
    - (맨하탄 지역의 직사각형 블록들을 가로-세로로 이동한다고 생각하면 이해 쉬움)
- 마할라노비스 거리(Mahalanobis distance)
    - 두 변수 사이에 높은 상관관계가 있을 때 우용
    - 변수 간 상관관계를 이용하여 구함
    - 계산이 복잡하다는 단점이 있음
        - 공분산행렬을 사용하는 것이 그 원인
- 데이터의 스케일이 클 수록 측정치에 미치는 영향이 클 수 있음
    - 표준화, 스케일러 등을 통해 해결할 수 있음

### 원-핫-인코더

- 텍스트, 범주형 데이터를 binary형태로 변환하여 모델링을 할 수 있다.
- **주의!** 선형회귀나 로지스틱회귀에서 원-핫-인코딩은 **다중공선성 관련 문제**를 일으킨다.
    - 한 가변수를 생략하는 방법이 있다.
    - KNN이나, 다른 방법에서는 이것이 큰 문제가 되지 않는다.

### 표준화(정규화, z 점수)

- 측정 시, 값 자체보다 '평균과 차이가 얼마나 나는지?'에 관심이 더 있을 수 있다.
- 표준화는 z 점수를 구하여, 변수들을 모두 비슷한 스케일에 놓는다.
    - z = (x값 - x평균) / s
    - → 평균으로부터 표준편차만큼 얼마나 떨어져있는가?
- KNN, PCA(Principle Component Analysis), Clustering 등에서는 표준화가 필수
- 표준화가 데이터의 분포를 정규분포를 만들어주는 것은 아님
- 꼭 표준화여야하는가?
    - 데이터를 0과 1 사이에 분포시키는 것이 항상 좋은 것은 아니다.
    - 또한 꼭 평균과 표준편자를 사용해야하는 것도 아니다.
        - 로버스트한 방법을 사용하기 위해 사분위수와 중앙값을 사용할 수도 있다.
    - 도메인 지식을 활용하여 데이터의 중요도를 가중해줄 수도 있다.
    - 무조건 표준화를 하는 것이 억지스러울 수 있다는 것을 인지해야한다!
        - 예를들어 범위가 1천만원~ 100억까지인데, 0과 1로 스케일을 줄여야 한다면 충분히 고려해야한다.

### K 선택하기

- 1-최근접 이웃 분류의 경우, 매우 직관적이지만 모델 성능이 대부분 좋지 않다.
- K>1일 때 성능이 좋으며, K가 너무 작거나 크면 아래와 같은 문제가 발생한다.
    - K가 너무 작으면, 노이즈값이 많이 고려되어, 오버피팅 위험이 있다.
    - 너무 크면, 결정함수가 너무 과하게 평탄화되어, 데이터의 지역 정보를 예측하는 KNN의 기능을 잃는다. (오버스무딩)
- 적합한 K는 문제에 따라 다름
    - 적합도는 테스트데이터에 대한 정확도를 통해 판단
    - 노이즈가 적고 잘 구조화된 데이터에서는 K가 낮으면 잘 동작
        - = 신호 대 잡음 비율(SNR, signal-to-noise ratio)이 낮을수록
        - ex. 손글씨, 음성 인식 데이터 등
    - 노이즈가 많은 데이터는 K가 클수록 좋음
        - ex. 대출 데이터 등
- 일반적으로는 1 < K ≤ 20로 놓으며, 동률이 나오는 경우를 막기 위해 일반적으로 홀수 사용함
- 편향-분산 트레이드오프(bias-variance tradeoff)
    - 오버스무딩과 오버피팅의 이율배반 관계
    - 다른 데이터셋을 사용할 때 결과가 크게 달라지거나, 실제 문제를 제대로 반영하지 못하는 것
    - 일반적으로 교차타당성검사 방법으로 해결

### KNN을 통한 피처엔지니어링

- 다른 분류 모델들에 사용할 수 있도록, '지역적 정보(local knowledge)'를 추가하는데에 사용 가능
    1. KNN은 데이터 기반으로 분류 결과(클래스에 속할 확률)을 얻는다.
    2. 이 결과는 해당 레코드에 새로은 피처로 추가된다
    3. 이 결과를 다른 분류 방법들에 사용한다.
        - 원래의 예측변수들을 두 번씩 사용하는 셈이다.
- 새롭게 만들어지는 피처는, 소수의 근접한 데이터로부터(K개) 얻는 지협적인 정보이기때문에
    - 다중공선성 문제가 발생하지 않는다.
    - 새로 얻는 정보들은 중복성이 있지 않고, 때로는 불필요하다.
- 이런 방법은 여러 분류 모델을 활용하는 것이므로 앙상블로 볼 수도 있고,
- 피처를 새로 만들기 때문에 피처엔지니어링이라고도 할 수 있다.

**주요 개념**

- KNN은 유사한 레코드들이 속한 클래스로 분류하는 방법
- 유클리드 거리나, 맨하탄 거리 등의 지표로 결정
- K의 수는 얼마나 좋은 성능을 보일지 결정
- 일반적으로 예측변수를 표준화하며,
    - 스케일이 큰 변수들의 영향력이 너무 커지지 않도록 한다.
- 예측 모델링의 첫 단계에서 종종 KNN을 사용하며, 이렇게 얻은 값을 다시 예측변수로 활용한다.

## 트리 모델

**용어 정리**

- 재귀 분할(recursive partitioning): 마지막 분할 영역에 해당하는 출력이 최대한 비슷한(homogeneous) 결과를 보이도록 데이터를 반복적으로 분할하는 것
- 분할값(split value): 분할값을 기준으로 예측변수를 그 값보다 작은, 큰 영역으로 나눔
- 노드(마디, node): 분할규칙 (네모)
- 잎(leaf): 마지막 가지 부분, 어떤 레코드에 적용할 최종적인 분류 규칙
- 손실(loss): 분류 과정에서 발생하는 오분류의 수. 손실이 많을수록 불순도가 높음
- 불순도(impurity): 데이터를 분할한 집합에서 서로 다른 클래스가 얼마나 섞여있는지 나타내는 지표. 많이 섞여일수록 불순도가 높다. (=이질성(heterogeneity), 반의어: 동질성(homogeneity, 순도)
- 가지치기(pruning): 학습이 끝난 모델에서 오버피팅을 줄이기 위해 가지를 하나씩 자르는 과정

---

- if-then-else 규칙의 집합체와 유사
- 회귀나 로지스틱 회귀와는 반대로, 트리는 데이터에 존재하는 복잡한 상호관계에 따른 **숨겨진 패턴을 발견**
- KNN이나 나이브 베이즈 모델과 달리, 예측변수들 사이의 관계로 단순 트리모델 표시 가능, 해석이 쉽다.

### 재귀분할

- 트리 모델에서 재귀 분할 알고리즘은
    1. 전체 데이터를 가지고 A를 초기화
    2. A를 두 부분 A1과 A2로 나누기 위해 분할 알고리즘 적용
    3. A1과 A2 각각에서 다시 반복
    4. 분할을 해도 분할 영역의 동질성이 개선되지 않으면, 알고리즘 종료
        - 각 영역은 클래스의 다수결로 예측결과 결정
        - 이진 결과 외에, 확률값을 구할수도 있다.
            - P(Y=1) = 영역 내 1의 수 / 영역 크기(1+0)
            - ex. P(Y=1) > 0.5인 경우, 해당 영역은 1로 예측

### 동질성과 불순도 측정

- 각 분할 영역에 대한 클래스 순도(class purity)를 측정할 방법 필요
    - 동질성, 불순도 측정을 통해.
    - 이는 0(완전)에서 0.5(순수 랜덤 추측) 사이의 값을 갖는다.
- 정확도는 불순도 측정에 좋지 않은 것으로 알려졌다.
    - 대신 **지니 불순도(Gini impurity)**, **엔트로피(entropy)**가 대표적인 불순도 측정 지표
    - 이 지표들은 클래스가 2개 이상인 분류 문제에서도 적용 가능
- 지니 불순도 계산
    - I(A) = p(1-p)
    - *지니 계수와 다르다!*
        - 지니 계수는 이진 분류로 한정되며, AUC지표와 관련 있는 용어
- 엔트로피 계산
    - I(A) = -p * log₂(p) - (1-p) * log₂(1-p)

### 트리 형성 중지하기

- 리프노드의 순도가 100%가 될 때까지 트리가 형성되면, 학습 데이터에 대해 정확도 100%를 보임
    - 하지만 이는 당연히 오버피팅된 결과이며,
    - 노이즈까지 학습한 결과이므로 새로운 데이터 분류를 방해함
- 가지치기
    - 오버피팅을 방지하기 위해, 트리모델에서는 직관적으로 가지치기를 함.
    - 일반적으로 홀드아웃 데이터에서 에러가 최소가 되는 지점까지 가지치기 진행
- 가지 분할을 멈추는 대표적인 방법 두 가지
    1. 리프의 크기 설정
        - 탐색 작업에는 유용할 수 있다.
    2. 복잡도 파라미터(complexity parameter)(깊이)
        - 트리의 복잡도를 조정
        - 새로운 데이터에 대한 예측 정확도를 최적화하는데 도움을 줌
        - 계수는 벌점의 개념으로, 계수가 클 수록 복잡도는 줄어듦

### 연속값 예측

- 알고리즘 논리는 분류와 동일하다.
- 다만
    - 하위 분할 영역에서 **평균으로부터 편차들을 제곱한 값을 이용해 불순도 측정**
    - **제곱근 평균제곱오차(RMSE)를 이용해 예측 성능을 평가**한다는 점이 다름

### 트리의 장점

- 시각화가 가능하며, 설명하기가 좋다.
- 변수 간 관계를 설명하기 쉽다.
- 하지만, 예측에 관해서는 다중 트리를 이용하는 것이 단일 트리를 이용하는 것보다 강력하다.
    - 특히 RF, Boosting 알고리즘은 거의 항상 우수한 정확도, 성능을 보여준다.
    - 하지만 위 언급한 장점들을 잃는다...

**주요 개념**

- 트리는 결과를 분류하거나 예측하기 위한 규칙들의 집합체를 생성한다.
- 규칙들은 데이터를 하위 영역으로 분할하는 것과 관련있다.
- 각 분할은 예측변수 값을 기준으로 데이터를 나누기 위함이다.
- 각 단계마다, 불순율이 최소화되는 쪽으로 진행한다.
- 더 이상 분할이 불가능하면 트리가 완전히 자랐다고 볼 수 있으며, 각 리프 노드 레코드는 단일 클래스에 속한다.
    - 하지만 완전히 자란 트리는 오버피팅하기때문에, 가지치기를 통해 이를 해결해야한다.
- RF나 Boosting 트리와 같은 다중 트리는 우수한 성능을 보장한다.
    - 하지만 규칙 기반을 둔 단일트리의 장점인 설명력, 전달력은 잃는다.(설명하기 어렵)

## 배깅과 랜덤 포레스트

**용어 정리**

- 앙상블(ensemble): 여러 모델의 집합을 이용해서 하나의 예측을 이끌어내는 방식 (=모델 평균화, model averaging)
- 배깅(bagging): 데이터를 부트스트래핑해서 여러 모델을 만드는 일반적인 방법 (= 부트스트랩 종합, bootstrap aggregation)
- 랜덤 포레스트(random forest): 의사 결정 트리 모델에 기반을 둔 배깅 추정 모델 (= 배깅 의사 결정 트리)
- 변수 중요도(variable importance): 모델 선응에 미치는 예측변수의 중요도

---

- 앙상블 방법의 가장 간단한 버전
    1. 예측 모델을 만들고 결과를 기록한다.
    2. 같은 데이터에 대해 여러 모델을 만들고 결과를 기록한다.
    3. 1-2를 통해 모은 결과들의 평규(혹은 가중평균, 다수결 투표)을 구한다.

### 배깅

- 배깅?
    - 부트스트랩 종합의 줄임말(bootstrap aggregating)
- 알고리즘
    1. 만들 모델의 수 M과 사용할 레코드의 수 n 의 값을 초기화, 반복변수 m=1로 선언
    2. 훈련 데이터로부터 복원추출 방법으로 n개의 부분 데이터 Ym과 **X**m을 부트스트랩 재표본 추출
    3. 의사결정규칙  fm^(**X**)를 얻기 위해, Ym과 **X**m을 이용해 모델을 학습
    4. m += 1로 모델 수 늘림. m ≤ M이면 다시 1단계로
- fm^이 Y=1인 경우의 확률을 예측한다고 했을 때, 배깅 추정치는 다음과 같다.
    - f^ = 1/M * (f₁(X) + f₂(X) + ... Fm^(X))

### 랜덤포레스트

- 알고리즘 로직
    1. 전체 데이터로부터 부트스트랩 샘플링(복원추출)
    2. 첫 분할을 위해 비복원 랜덤표본추출로 p개의 변수 샘플링
    3. 샘플링된 변수 Xf(1)...Xj(p)에 대해 알고리즘 적용
        - Xj(k)의 각 변수 sj(k)에 대해
            - 파티션 A에 있는 레코드들을 Xj(k) < sj(k)인 하위 영역과 Xj(k) ≥ sj(k)인 하위 영역으로 나눔
            - A의 각 하위 영역 내부의 클래스의 동질성을 측정
        - 분할 영역 내부의 클래스 동질성을 최대로 하는 sj(k)값을 선택
    4. 분할영역 내부의 클래스 동질성을 최대로 하는 Xj(k)와 sj(k)값을 선택
    5. 다음 분할을 진행하기 위해, 2단계부터 시작해 이전 단계들을 반복
    6. 트리가 모두 자랄 때까지 위와 같은 분할 과정을 반복
    7. 1단계로 돌아가 또 다른 부트스트랩 표본을 추출해 같은 과정 반복
- 샘플링해야하는 변수의 수는?
    - 변수의 개수가 P개일 때, P 제곱근개 정도를 선택
- 주머니 외부(OOB, out-of-bag) 추정 에러는 트리 모델을 만들 때 사용했던 학습 데이터에 속하지 않는 데이터를 사용해 구한 학습된 모델의 오차율
    - rf에서 트리가 추가될 때마다 오차율이 감소 (일정 수준부터는 유지)

### 변수 중요도

- rf로 변수 중요도 측정하는 방법
    1. 변수의 값을 랜덤하게 섞었다면, 모델의 정확도가 감소하는 정도를 측정
    (모델 정확도)
        - 변수를 랜덤하게 섞는다 = 해당 변수가 예측에 미치는 모든 영향력을 제거한다
        - 정확도는 OOB 데이터로부터 얻는다. (교차타당섬검사와 같은 효과)
    2. 특정 변수를 기준으로 분할이 일어난 모든 노드에서 불순도 점수의 평균 감소량을 측정
    (지니불순도)
        - 변수가 노드의 순도를 개선하는데 얼마나 기여하는지 나타냄
        - 학습 데이터 기반이기때문에, OOB 데이터로 계산한 것에 비해 신뢰도 떨어짐
- 근데, RF에서는 지니 불순도만 활용함.
    - 정확도를 계산하려면 계산 복잡도가 올라감
    - 반면, 지니 불순도는 알고리즘 상 부차적으로 얻어지는 결과물, 추가 계산 필요 없음

### 하이퍼파라미터

> 1) 파라미터: 모델링 후, 데이터를 통해 산출되어지는 값
ex. 선형회귀에서 결정계수(w)
2) 하이퍼파라미터: 사용자가 모델 성능을 위해 입력하는 값, 외부의 값
ex. KNN에서 K

- 모델의 성능을 조절하기 위해 하이퍼파라미터를 조정할 수 있다.
    - nodesize: 리프노드의 크기
    - maxnodes: 노드의 최대 수(깊이)
- 귀찮을 수 있지만, 오버피팅을 방지하기 위해 최적의 파라미터를 찾아내야한다.

**주요개념**

- 앙상블 모델은 여러 모델로부터 얻은 결과를 결합하여 모델 정확도를 높음
- 배깅은 앙상블 모델의 하나로, 부트스트랩 샘플을 이용해 많은 모델 생성, 이 모델들을 평균화
- 랜덤 포레스트는 배깅 기법을 의사 결정 트리 알고리즘에 적용한 형태
    - 데이터 재표본추출하는 동시에, 트리 분할 시 예측변수 또한 샘플링(일부만 사용)
- 랜덤 포레스트로부터 나오는 충요한 출력 중 하나는 변수 중요도
- RF는 오버피팅을 피하기 위해 교차타당성검사를 통해 조정된 하이퍼파라미터 사용

## 부스팅

**용어 정리**

- 앙상블(=모델 평균화): 여러 모델들의 집합을 통해 예측 결과를 만들어내는 것
- 부스팅(boosting): 연속된 라운드마다 잔차가 큰 레코드들에 가중치를 높여 일련의 모델들을 생성하는 방법
- 에이다부스트(AdaBoost): 잔차에 따라 데이터의 가중치를 조절하는 부스팅의 초기 버전
- 그레이디언트 부스팅(gradient boosting): 비용함수(cost function)를 최소화하는 방향으로 부스팅을 활용하는 형태
- 확률적 그레이디언트 부스팅(stochastic -): 각 라운드마다 **레코드와 열을 재표본추출하는 것을 포함**하는 부스팅의 가장 일반적인 형태
- 정규화(regularization): 비용함수에 모델의 파라미터 개수에 해당하는 벌점 항을 추가해 오버피팅을 피하는 방법 (앞장들에서 나오는 정규화와 다름)

---

- 부스팅은 배깅보다 더 많은 부가 기능을 갖고있고, 주의가 필요하다.
- 선형회귀에서 피팅이 개선될 수 있는지 잔차를 사용했던 아이디어를 발전시켜,
- 이전 모델이 갖는 오차를 줄이는 방향으로 다음 모델을 연속적으로 생성한다.
    - 에이다부스트
    - 그레이디언트 부스팅
    - 확률적 그레이디언트 부스팅
    - 이렇게 자주 쓰이며, 특히 확률적 그레이디언트 부스팅이 자주 쓰인다.

### 부스팅 알고리즘

- 에이다 부스팅의 자세한 알고리즘은 p.267 참고
    - 잘못 분류된 관측 데이터에 가중치를 증가시켜, 성능이 제일 떨어지는 데이터에 집중 학습하는 효과
- **그레이디언트 부스팅**은 에이다 부스팅과 비슷하지만,
    - 비용함수를 최적화하는 접근법을 사용한다는 점에서 차이 있음
    - 가중치를 조정하는 대신, 유사잔차(psudo-residual)를 학습하도록 함
    - 잔차가 큰 데이터를 더 집중적으로 학습하는 효과
- **확률적 그레이디언트 부스팅**은 랜덤 포레스트에서와 유사하게,
    - 매 단계마다 데이터와 예측변수를 샘플링하는 식
    - 그레이디언트 부스팅에 랜덤 요소 추가

### XG부스트

- 확률적 그레이디언트 부스팅을 구현한 것
    - 여러 옵션이 효율적으로 구현되어있고,
    - Python, R을 비롯한 대부분의 데이터 과학 소프트웨어 언어를 지원
- 다양한 하이퍼파라미터 중 중요한 파라미터는
    - subsample
        - 각 반복 구간마다 샘플링할 입력 데이터의 비율을 조정
        - 이 설정에 따라 비복원 추출로 샘플링한다는 점만 빼면, 부스팅은 마치 rf처럼 동작
    - eta
        - a(m)에 적용되는 축소 비율을 결정
        - 가중치의 변화량을 낮춰, 오버피팅을 방지하는 효과가 있음

### 정규화: 오버피팅 피하기

- 정규화(regularization)
    - 모델의 복잡도에 따라 벌첨을 추가하는 형태로 비용함수를 변경하는 방법
- XG부스트에서는 alpha와 lambda로 조정
    - alpha
        - 맨하탄 거리
    - lambda
        - 유클리드 거리
    - 이 파라미터들을 크게 하면, 모델이 복잡해질수록 많은 벌점을 부여
    - 결과적으로는 얻어지는 트리의 크기가 작아지게 된다.

### 하이퍼파라미터와 교차타당성검사

- 주로 오버피팅, 정확도, 계산 복잡도 사이의 균형을 잡아주기 위해 사용됨
- eta
    - 부스팅 알고리즘에서 a에 적용되는 0과 1 사이의 축소인자(shrinkage factor)
    - 노이즈가 있는 데이터에 대해서는 기본값보다 적은 값 추천
- nrounds
    - 부스팅 라운드 횟수
    - eta가 작은 값이라면 알고리즘의 학습 속도가 늦춰지므로, 라운드 수를 늘려야함.
    - 오버피팅을 방지하는 파라미터 설정이 포함된 경우, 라운드 수를 좀 더 늘려도 괜찮음
- max_depth
    - 트리의 최대 깊이
- subsample, colsample_bytree
    - 전체 데이터에서 일부 데이터를 비복원 샘플링하는 비율 및 예측변수 중 일부 변수를 샘플링하는 비율
    - 랜덤 포레스트에서 오버피팅을 피하기 위해 사용했던 것들과 유사
- lambda 및 alpha
    - 오버피팅을 조절하기 위해 사용되는 정규화 파라미터들(벌점)

**주요 개념**

- 부스팅 방법은 일련의 모델들을 피팅할 때, 이전 라운드에서 오차가 컸던 레코드에 가중치를 더하는 방식을 사용하는 앙상블 모델
- 확률적 그레이디언트 부스팅은 부스팅 가운데에서 가장 일반적으로 사용되며, 가장 높은 성능
    - 일반적인 형태는 트리 모델을 사용
- XG부스트는 확률적 그레이디언트 부스팅을 사용하기 위한 가장 유명한 패키지
- 부스팅은 데이터에 오버피팅되기 쉬우며, 이를 방지하기 위해 하이퍼파라미터를 잘 조정해야함
- 정규화는 파라미터 개수(ex. 트리 크기)에 관한 벌점 항목을 모델링에 포함하여 오버피팅을 피하는 방법
- 하이퍼파라미터들의 조합을 찾을 때에는 교차타당성 검사가 중요
