# 회귀와 예측

## 회귀와 예측

- 변수 X와 변수 Y가 서로 관련이 있는가?
- 있다면 어떤 관련이 있는가?
- 이를 이용해 Y를 예측할 수 있는가?

> 단순선형회귀

- 한 변수와 또 다른 변수의 크기 사이에 어떤 관계가 있는지 보여줌
    - X가 증가하면 Y도 증가, 혹은 X가 증가하면 Y는 감소?
- 상관관계와 다른 점
    - 상관관계
        - 두 변수 사이의 전체적인 강도를 측정하여 상관계수로 보여줌
    - 단순선형회귀
        - 관계 자체를 정량화

### 회귀식

- *Y = b₀ + b₁X*
- 머신러닝 분야에선
    - *Y*는 목표벡터, *X*는 피처벡터

### 적합값과 잔차

- 모든 데이터가 정확히 한 직선에 들어오지 않음
    - 회귀식은 명시적으로 오차항 *eᵢ*를 포함함

- *Yᵢ = b₀ + b₁Xᵢ  + **eᵢ***
- 적합값: 예측값, y^(Y햇) 이라고 나타냄.
- *Yᵢ^ = b₀^ + b₁^Xᵢ*
    - *b₀^*과  *b₁^*은 알려진 값이 아닌, 추정을 통해 얻은 값이라는 의미
- 잔차: 예측값에서 원래 값을 빼서 구함
    - *eᵢ^ = Yᵢ - Yᵢ^*

### 최소제곱

- 회귀선은 잔차들을 제곱하여 더한 값, 잔차제곱합(RSS: residual sum of squares)을 최소화하는 선
    - 추정치 *b₀^*과  *b₁^*은 RSS를 최소화하는 값 (최소제곱법)
- RSS를 최소화하는 방법을 최소제곱회귀, 보통최소제곱(OLS: ordinary least squares)회귀라고 함
- 최소제곱은 특잇값에 매우 민감
    - 선형으로 나타나는데, 특잇값이 있다면 잔차가 커지고, 최소제곱은 제곱으로 커지니까

### 예측 대 설명

- 회귀는 예측, 설명에 모두 사용된다.
- 회귀방정식 자체는 상관관계는 나타내주지만, 인과관계를 설명하지는 않는다.
- 인과관계는 그에 해당하는 지식을 발휘

> 다중선형회귀

- 예측변수가 여러개라면 더 이상 직선의 형태는 아니지만, 여전히 선형모형
- 제곱근 평균제곱오차(RMSE: root mean squared error)
    - 회귀 시 평균제곱오차의 제곱근 - 회귀모형을 평가하는데 가장 널리 사용되는 측정 지표
- 잔차 표준오차(RSE: residual standard error)
    - 평균제곱오차와 동일하지만 자유도에 따라 보정된 값
    - 빅데이터에선 RMSE랑 큰 차이 x
- R제곱(r-squared, 결정계수)
    - 0에서 1까지 모델에 의해 설명된 분산의 비율
- t통계량(t-dtatistic)
    - 계수의 표준오차로 나눈 예측변수의 계수. 모델에서 변수의 중요도를 비교하는 기준
- 가중회귀
    - 다른 가중치를 가진 레코드들을 회귀하는 방법

### 모형 평가

- 데이터 과학에서 가장 중요한 성능지표는 제곱근 평균제곱오차(RMSE)
    - 예측된 *Yᵢ^*값들의 평균제곱오차의 제곱근 *((y - y^)제곱 평균의 제곱근)*
- 잔차 표준오차(RSE)는 평균제곱오차와 같지만, 분모가 자유도.
    - 하지만 자유도로 나누나 n으로 나누나 빅데이터에선 별 차이 없음
- R제곱
    - 모델이 데이터에 얼마나 적합한지 평가하고자 할 때, 회귀분석을 설명하기 위한 용도로 활용
    - 1 - (sum(y-y^))² / (sum(y-y기댓값))²  > 분모는 y의 분산에 비례
- t통계량 그리고 p값
    - t통계량이 높을수록, p값이 낮을수록 예측변수(피처)는 더욱 유의미

### 교차타당성검사

- 홀드아웃 샘플
    - 데이터를 두 개로 나누고, train은 모델을 만드는 데 쓰고, test는 모델 성능 테스트에 사용
- **교차타당성검사**
    - 홀드아웃 샘플 아이디어를 여러 개의 연속된 홀드아웃 샘플로 확장한 것.
- k 다중 교차타당성 검사 알고리즘 (k fold cross validation)
    1. 1/k의 데이터를 홀드아웃 샘플로 따로 떼어놓는다.
    2. 남아있는 데이터로 모델을 훈련시킨다.(train)
    3. 모델을 1/k 홀드아웃에 적용 (점수를 매김)하고 필요한 모델 평가 지표를 기록한다.
    4. 데이터의 첫 번째 1/k을 복원하고 다음 1/k(앞에서 선택했던 레코드 제외)을 따로 보관
    5. 2~3단계 반복 
    6. 모든 레코드가 홀드아웃 샘플로 사용될 때까지 반복
    7. 모델 평가 지표들을 평균과 같은 방식으로 결합
- 폴드(fold)?
    - 훈련을 위한 샘플과 홀드아웃 샘플로 데이터를 나누누는 것

### 모형 선택 및 단계적 회귀

- 더 많은 예측변수(X)를 추가한다고 더 좋은 모델을 얻는 것은 아니다.
- **오컴의 면도날**
    - 모든 것이 동일한 조건에서는, 복잡한 모델보다는 단순한 모델을 우선 사용한다는 원리
- 변수를 추가하면 항상 RMSE는 감소하고 R스퀘어는 증가
    - 따라서 이렇게 추가하는 변수들은 모델 선택에 별 도움을 주지 않는다.
- 모델에 항(변수, 예측변수)을 추가할수록 불이익을 주는 AIC라는 측정 기준도 있음.
(Akaike's information criteria)
    - 레코드의 수(*k*) * 2 만큼 불이익을 받게됨
- AIC를 최소화하는 모델을 찾는 방법
    - 부분집합회귀 (표본 내 방법)
        - 모든 가능한 모델을 검색
        - 계산 비용이 많이 들며, 대용량에 적합하지 않다.
    - 단계적 회귀 (표본 내 방법)
        - 예측변수를 연속적으로 추가/삭제하여 AIC를 낮추는 모델 찾음
    - 전진선택(forward selection)
        - 예측변수 없이 시작하여, 각 단계에서 R²에 가장 큰 기여도를 갖는 예측변수 추가
        - 기여도가 통계적으로 더 이상 유의미하지 않을 때 중지
    - 후진선택(backward selection, =후진제거)
        - 전체 모델로 시작, 유의미한 모델이 될 때까지 유의하지 않은 예측변수 제거

- 벌점회귀(penalized regression)
    - 개념적으론 AIC와 같음
    - 개별 모델 집합들을 명시적으로 검색하는 대신, 모델 적합 방정식에 많은 변수(파라미터)에 대해 모델에 불이익을 주는 제약조건을 추가한다.(벌점)
    - 단계적, 전진, 후진 선택처럼 변수를 제거하지는 않고, 계수 크기를 감소시키거나 경우에 따라 거의 0으로 만들어 벌점을 적용.
    - 방법으로는 **능형회귀**와 **라소**가 대표적.

### 가중회귀

- 서로 다른 관측치를 다른 정밀도로 측정하였을 때, 역분산 가중치를 얻을 수 있다.
- 가중치 변수가 집계된 데이터의 각 행이 나타나는 원본 관측치의 수를 인코딩하도록 집계된 형식의 데이터를 분석할 수 있다.
- 쉽게말해서, 레코드별로 가중치를 주기 위해 사용

> 회귀를 이용한 예측

- 데이터 과학에서 회귀의 목적은 예측
- 전통적 통계에서 회귀는 예측보다는 설명을 위한 모델링에 적합했었음.
- 외삽법: 모델링에 사용된 데이터의 범위를 벗어난 부분까지 모델을 확장하는 것

### 외삽의 위험

- 회귀모형을 데이터 범위를 초과하면서까지 외삽하는데 사용해서는 안됨.

### 신뢰구간과 예측구간 (p.169 이해 안돼서 일단 패스)

- 가장 일반적으로 마주치는 회귀 관련 신뢰구간은
    - 회귀 파라미터(계수)의 신뢰구간
- 신뢰구간
    - 회귀계수 주변의 불확실성을 정량화
- 예측구간
    - 개별 예측값의 불확실성을 정량화
    - 예측구간이 신뢰구간보다 통상적으로 넓다.
    - 신뢰구간은 어쨋든 통계량에 대한 것이고, 예측구간은 하나의 값에 대한 불확실성과 관련있기 때문
- 수식 대신 부트스트랩을 사용할 수 있다.

### 회귀에서의 요인변수(= 범주형 변수)

- 범주형 변수라고도 하는 요인변수는 개수가 제한된 이산값을 취함
    - ex. 아파트 이름: 한양, 현대, 레미안, 효성 등등..
- 지표변수는 0 혹은 1의 값을 갖는다. (=이진변수, binary)
- 회귀분석에서는 수치 입력이 필요하므로, 범주형 변수를 수치화해야 한다.
- 일반적으로 가변수들의 집합으로 변환한다.
- 가변수(dummy variable)
    - 회귀나 다른 모델에서 요인 데이터를 사용하기 위해 0과 1의 이진변수로 부호화
- 기준 부호화(reference coding, 처리 부호화)
    - 통계학자들이 많이 쓰는 부호화 형태.
    - 한 요인을 기준으로 하고 다른 요인들이 이 기준에 따라 비교할 수 있도록 함
- 원핫 인코딩
    - 머신러닝 분야에서 많이 사용되는 부호화
    - 모든 요인 수준이 계속 유지됨
    - **최근접 이웃** 알고리즘, **트리모델**과 같은 알고리즘에서 많이 사용
    - 다중선형회귀에서는 부적합

- 편차 부호화(=총합 대비)
    - 기준 수준과는 반대로 전체 평균에 대해 각 수준을 비교하는 부호화

- 가변수 표현
    - 회귀분석에서 P개의 개별 수준을 갖는 요인변수는 보통 P-1개의 열의 행렬로 표현
        - 다중공선성 오류 발생을 방지하기 위해.
    - 요인변수를 P개의 개별 값으로 인코딩하기 위한 가장 흔한 방법은 P-1개의 가변수를 만들어 사용하는 것
- 다수의 수준을 갖는 요인변수들
    - 잔차를 사용...p.174
    - 더 적은 수의 수준을 갖는 변수가 되도록 수준들을통합
        - 개별 우편번호에서 도시 우편번호로 변환하는 등
- 순서가 있는 요인변수
    - 수치형 변수로 변환
        - A: 13, B: 12, C: 11 등

### 회귀방정식 해석

- 데이터 과학에서 회귀의 가장 중요한 용도는 결과변수를 예측하는 것
- 하지만 예측변수와 결과 간 관계의 본질을 이해하기 위해 방정식 자체로 통찰을 얻어야 할 때도 있음
- 변수 간 상관
    - 예측변수들끼리 서로 높은 상관성을 가질 때, 개별 계수를 해석하는 것은 어렵다
- 다중공선성
    - 예측변수들이 완벽 혹은 거의 완벽에 가까운 상관성을 가지면 회귀는 불안정하여 계산 불가
        - 오류로 인해 한 변수가 여러 번 포함
        - 요인변수로부터 P-1개가 아닌 P개의 가변수가 만들어진 경우
        - 두 변수가 서로 거의 완벽하게 상관성이 있는 경우
    - 회귀분석에서는 다중공선성을 반드시 해결해야함.
    - 트리, 클러스터링, 최근접 이웃 알고리즘 등 회귀유형이 아닌 방법에서는 그다지 문제가 되지 않음.
        - 이들 방법에서는 P-1개 대신 P개의 가변수를 유지하는 것이 좋다.
- 교란변수
    - 중요한 예측변수이지만 회귀방정식에 누락되어 결과를 잘못되게 이끄는 변수

- 주효과
    - 다른 변수들과 독립된 하나의 예측변수와 결과변수 사이의 관계
- 상호작용
    - 둘 이상의 예측변수와 응답변수 사이의 상호 의존적인 관계
- 상호작용 항들을 이용한 모델 선택
    - 다수의 변수가 존재하는 문제의 경우, 모델에서 어떤 상호작용을 고려해야 할지 결정하기가 어려움
        - 어떤 문제에서는 사전 지식이나 직관이 도움을 준다.
        - 단계적 선택을 사용해서 다양한 모델들을 걸러낸다.
        - 벌점을 부여하는 방식의 회귀방법을 사용하여 자동으로 가능한 상호작용을 가려내도록 한다.
        - 랜덤 포레스트, 그레이디언트 부스팅 트리 같은 트리 모델을 사용하여 최적의 상호작용 항들을 걸러낸다.
    - 변수와 결과가 서로 의존적일 때, 두 변수 사이의 상호작용을 고려할 필요가 있다.

### 가정 검정: 회귀 진단

용어

- 표준화잔차(standardized resicual): 잔차를 표준오차로 나눈 값
- 영향값(influential value): 있을 때와 없을 때 회귀방정식이 큰 차이를 보이는 레코드
- 지렛대(leverage, =hat value): 회귀식에 한 레코드가 미치는 영향력의 정도
- 비정규 잔차(non-normal residual): 정규분포를 따르지 않는 잔차는 회귀분석의 요건을 무효로 만들 수 있음 (데이터사이언스에서는 별로 중요하지 않음)
- 이분산성: 어떤 범위 내 출력값의 잔차가 매우 높은 분산을 보이는 경향 (어떤 예측변수를 회귀식이 놓치고 있다는 것을 의미할 수 있음)
- 편잔차그림(partial residual plot): 결과변수와 특정 예측변수 사이의 관계를 진단하는 그림

---

- 특잇값
    - 표준화잔차를 조사하여 특잇값을 발견할 수 있음 ( 잔차 / 표준오차)
    - 특잇값을 분류하는 통계 이론은 따로 없으며, 임의의 경험칙이 존재함
        - 예를들어, boxplot을 활용한 아웃라이어 판별 → 사분위 범위 * 1.5
        - 표준화잔차 → 회귀선으로부터 떨어진 정도를 표준오차 개수로 표현한 값
    - 특잇값이 새로운 값 예측에 큰 영향을 끼치지는 않는다.
        - 대신, 특잇값 검출 시에는 매우 중요하다.
        - 특잇값은 갑작스러운 사건 발생과도 연관이 있을 수 있으며, 도메인 지식을 통한 특잇값 해석은 아주 중요한 사업의 가치가 될 수도 있다.
- 영향값
    - 데이터가 클 경우, 한 레코드가 모델을 변화시키는 경우는 적다.

- 이분산성, 비정규성, 오차 간 상관
    - 데이터 과학 관점에서, 잔차 분포에 대해 그리 큰 신경을 쓰지는 않아도 됨

- 편잔차그림과 비선형성
    - 예츩변수와 결과변수 간 관계를 얼마나 잘 설명하는지 시각화하는 방법
    - 특잇점 검출과 함께 데이터 과학자들에게 가장 중요한 모델 진단 방법

    편잔차 = 잔차 + b^ * X    # b^: 회귀계수 추정치

### 다항회귀와 스플라인 회귀

용어

- 다항회귀(polynomial regression): 회귀 모형에 다항식(제곱, 세제곱)추가한 방식
- 스플라인 회귀(spline regression): 다항 구간들을 부드러운 곡선으로 피팅
- 매듭(knot): 스플라인 구간을 구분하는 값
- 일반화가법모형(GAM, generalized additive model): 자동으로 구간을 결정하는 스플라인모델

---

- 응답변수와 예측변수 간 관계가 반드시 선형일 필요는 없음
    - ex. 마케팅 비용과 마케팅 효과 (초기엔 효과가 크지만 나중엔 차이가 미미)
- 다항식

    Y = b0 + b1X + b2X²

- 스플라인 회귀
    - 다항회귀는 곡률을 담아낼 수 있음
    - 하지만, 다항회귀는 바람직하지 않은 흔들림을 초래
    - 스플라인은 고정된 점들 사이를 부드럽게 보간하는 방법
    - 스플라인은 일련의 조각별 연속 다항식
    - 하지만 스플라인이 항상 다항회귀보다 좋은 것은 아님
- 일반화가법모형
    - 스플라인 회귀를 자동으로 찾는 기술
        - 다항 항은 관계 포착에 있어 유연성 부족할 수 있고
        - 스플라인은 매듭을 직접 지정해야한다.

### 주요 개념

- 회귀분석에서 아웃라이어는 잔차가 큰 레코드
- 다중공선성은 회귀 방정식 피팅 시 불안정성 가져옴 → 무조건 해결해야함
- 교란변수는 모델에서 생략된 중요한 예측변수
    - 허위 관계를 보여주는 회귀 결과를 낳을 수 있다.
- 한 변수의 효과가 다른 변수의 수준에 영향을 받는다면, 두 변수 사이의 상호작용을 고려할 항이 필요함.
- 다항회귀분석은 예측변사와 결과변수 간 비선형 관계를 검증할 수 있다.
- 스플라인은 매듭들로 함께 묶여있는 일련의 구간별 다항식을 말한다.
- 일반화기법모형(GAM)은 스플라인의 매듭을 자동으로 결정하는 프로세스를 가지고있다.